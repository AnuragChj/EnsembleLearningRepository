Our DT Classifier allows the user to create a decision tree and predict which classes observations belong to. 
There are hyperparameters such as min_samples_split and max_depth, as well as loss_func, which allows the user to specify whether they'd prefer gini loss or entropy loss to determine the splits. 
The splits of each node are determined using simple iterative parsing for each feature and each unique value of that feature. The algorithm tests out which mid point between 2 neighboring feature values are the best, and which feature is the best, for each node, in order to split the tree. The best split is chosen depending on the maximization of impurity loss, weighted between both left and right child nodes.
The tree is trained recursively. In the train function, the find_split function is called, which calculates the best split value and feature for each node, then assigns a dataframe of observations less than that value to the node's left child node, and a dataframe of observations more than or equal to that value to that node's right child node. Then, the train function is called on the left and right child nodes again. This way, the tree automatically recursively grows and splits each node into child nodes, and child nodes into grandchild nodes, et cetera.
At each split, the depth of the tree is constantly updated, such that if it hits the maximum depth, the tree stops growing. Alternatively, the tree also stops growing if min samples of each node is not met.

When tested out using sklearn's accuracy_score, the tree performed very well on the iris dataset, with a 97.7% accuracy on the test set.
