import numpy as np
import pandas as pd

class DecisionTreeClassifier:
  def __init__(self, Y: list, X: pd.DataFrame, min_samples_split=None, # This will represent the root node 
               max_depth=None, depth=None, nodetype=None):  # But same attributes will be re-used for child nodes
    
    # initialize left and right nodes as none, since the first instance will be the root node
    self.left = None
    self.right = None

    # assigning arguments into instance attributes of the node (default max depth is 10, default min samples is 25)
    self.X = X
    self.Y = Y
    self.features = list(self.X.columns)
    self.md = max_depth if max_depth != None else 10
    self.mss = min_samples_split if min_samples_split != None else 25
    self.nodetype = nodetype if nodetype != None else 'root node'


    # we need this later to recursively add leaves to the tree and monitor its depth
    self.depth = depth if depth != None else 0 

    # count of observations falling under each unique label, and sort them in ascending
    self.counts = dict(self.Y.value_counts())
    self.sorted_counts = list(sorted(self.counts, key = lambda x:x[1])) # sort by value count, not by label name

    # get gini score
    self.gini = self.gini_() ## Define gini_impurity function later down

    # majority class of each node (also the predicted class for a node with no more children nodes)
    # Get the majority class if the node has at least 1 class, else return None
    self.y_pred = self.sorted_counts[-1][0] if len(self.sorted_counts) > 0 else None 

    # number of obs in a node
    self.n = len(Y)

    # list of unique classes
    self.classes = sorted(list(dict.fromkeys(list(Y))))

  #function to get the gini of a node
  def gini_(self): 
    
    class_counts = []
    for i in self.counts.items():
      class_counts.append((i[0], i[1])) # e.g. [('spam', 747), ('ham', 4825)]
    
    counts_list = [class_counts[i][1] for i in range(len(class_counts))]

    return self.calculate_gini(counts_list) # e.g. [747, 4825]


  #function to get the gini of a list of class counts
  @staticmethod
  def calculate_gini(counts_list): 

    n = sum(counts_list)

    if n == 0:
        return 0.0

    # probabilities of each class
    probs = []
    for i in counts_list:
      probs.append(i/n)
    
    # gini formula 
    sum_probs = []
    for i in probs:
      sum_probs.append(i**2)

    sumprobs = sum(sum_probs)

    gini = 1 - sumprobs
    
    return gini

  # finds the best split given features and classes, using max gain of gini impurity algorithm
  def find_split(self):

    data = self.X.copy()
    data['classes'] = self.Y
    parent_gini = self.gini_()
    gain = 0 # initialize gain (change in gini impurity) as zero

    # initialize best feature and best value as none
    best_feature = None
    best_value = None

    # iterate over each feature to find best feature for maximizing gini gain
    for i in self.features:
        # drop NAs
        X_data = data.dropna()

        # sort by feature values in ascending order
        X_data = X_data.sort_values(i) 

        # get the mid (average) between 2 neighboring numerical values of the same feature
        x_mid = list(pd.Series(X_data[i].unique()).rolling(2).mean())[1:]

        # iterate over each average of 2 neighboring values to find best split within this feature for maximizing gini gain
        for j in x_mid:

            left_counts = dict(X_data[X_data[i]<j]['classes'].value_counts()) # e.g. {class1 : 747, class2: 4825}
            right_counts = dict(X_data[X_data[i]>=j]['classes'].value_counts())

            left_list = [k[1] for k in left_counts.items()] # e.g. [747, 4825]
            right_list = [k[1] for k in right_counts.items()]
            left_gini = self.calculate_gini(left_list)
            right_gini = self.calculate_gini(right_list)

            weighted_gini = (
                (left_gini*sum(left_list))/(sum(left_list)+sum(right_list))) + (
                    (right_gini*sum(right_list))/(sum(left_list)+sum(right_list))
                    ) # just the formula to get the weighted gini
            
            gini_gain = parent_gini - weighted_gini # getting the gini gain

            # Checking if this is the best split so far 
            if gini_gain > gain:
                split_feature = i
                split_value = j 
                gain = gini_gain
            else:
              pass

    return (split_feature, split_value)
