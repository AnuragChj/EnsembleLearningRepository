{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Copy of nlpex3-anurag-chatterjee-hugo-vanderperre-xinyu-hu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnuragChj/EnsembleLearningRepository_Anurag_Vidhi_Jian_Sampurna/blob/main/Copy_of_nlpex3_anurag_chatterjee_hugo_vanderperre_xinyu_hu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install sacrerouge sacrebleu bert-score\n",
        "\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!pip install ./transformers/."
      ],
      "metadata": {
        "trusted": true,
        "id": "TUBKzziacVL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary libraries\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "from datasets import load_dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "trusted": true,
        "id": "JPOx4k44cVL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading dataset\n",
        "train_dataset = load_dataset('xsum', split='train')\n",
        "valid_dataset = load_dataset('xsum', split='validation')\n",
        "test_dataset = load_dataset('xsum', split='test')\n",
        "\n",
        "# in this example we will use only one batch containing 10 examples \n",
        "batch_input = test_dataset[\"document\"][0:10]\n",
        "batch_output = test_dataset[\"summary\"][0:10]\n",
        "\n",
        "# Each model has a name on the hugging face website: you can search through the list of all models here https://huggingface.co/models\n",
        "model = BartForConditionalGeneration.from_pretrained(\"sshleifer/distilbart-xsum-6-6\")\n",
        "\n",
        "# usually each model has a special tokenizer these tokenizers contain the vocabulary dictionary of all the tokens you should find the one that works with the model you use\n",
        "# usually they have the same name but for the model \"sshleifer/distilbart-xsum-6-6\" which is made by the community we know that it is an adaptation of BART model so \n",
        "# it works with the BART tokenizer.\n",
        "tok = BartTokenizer.from_pretrained(\"facebook/bart-base\")  ## tokenizer\n",
        "\n",
        "# using our loaded tokenizer: we will encode the input documents into \n",
        "# max_length is 1024 as the bart model allows to accept 1024 tokens max as an input \n",
        "input_encodings = tok.batch_encode_plus(batch_input, padding=True, max_length=1024, truncation=True, return_tensors=\"pt\")\n",
        "target_encodings = tok.batch_encode_plus(batch_output, padding=True, max_length=1024, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "\n",
        "# Given the batch Decode the answer from your model note that model.generate function takes many params, it will operate as greedy decoding if none is given. \n",
        "model_output = model.generate(input_encodings[\"input_ids\"])\n",
        "\n",
        "## converting model output into text (this is called tokenizer \"decoding\") (not to confuse with decoding algorithms like sampling and beam search these have the function model.generate)\n",
        "model_output_decoded = tok.batch_decode(model_output, skip_special_tokens=True)\n",
        "for s,t,g in zip(batch_input, batch_output, model_output_decoded):\n",
        "  print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated\\t{}\\n------\\n\".format(s[0:1000],t,g))"
      ],
      "metadata": {
        "trusted": true,
        "id": "mpFJgbdYcVL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.1**"
      ],
      "metadata": {
        "id": "YKDs56BLcVMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.1:Translation**"
      ],
      "metadata": {
        "id": "ynotyBQgcVMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bible-para Dataset**"
      ],
      "metadata": {
        "id": "2NRh7DkrcVMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset = load_dataset(\"bible_para\",lang1 = 'en', lang2 = 'fr', split = \"train[:20%]\")\n",
        "trans_bible_eval = valid_dataset[0:1000]"
      ],
      "metadata": {
        "trusted": true,
        "id": "JEQK8te5cVMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model for bible_para dataset\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Tokenizer\n",
        "tok = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "batch = valid_dataset['translation'][0:10]\n",
        "input_sequences = [i['en'] for i in batch]\n",
        "batch_output = [i['fr'] for i in batch]\n",
        "\n",
        "task_prefix = \"translate English to French: \"\n",
        "encoding = tok(\n",
        "    [task_prefix + sequence for sequence in input_sequences],\n",
        "    padding=\"longest\",\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = encoding.input_ids\n",
        "model_outputs = model.generate(input_ids)\n",
        "\n",
        "## converting model output into text (this is called tokenizer \"decoding\") (not to confuse with decoding algorithms like sampling and beam search these have the function model.generate)\n",
        "model_output_decoded = tok.batch_decode(model_outputs, skip_special_tokens=True)\n",
        "for s,t,g in zip(input_sequences, batch_output, model_output_decoded):\n",
        "  print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated\\t{}\\n------\\n\".format(s[0:1000],t,g))"
      ],
      "metadata": {
        "trusted": true,
        "id": "tYxA0RIdcVMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**un-multi dataset**"
      ],
      "metadata": {
        "id": "V5lxU_AscVMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset = load_dataset(\"ted_iwlst2013\", \"en-fr\", split = \"train[:1%]\")\n",
        "trans_unmulti_evel = valid_dataset[0:1000]"
      ],
      "metadata": {
        "trusted": true,
        "id": "JTyjtKEncVMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the pre-trained model for bible_para dataset\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Tokenizer\n",
        "tok = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "batch = valid_dataset[\"translation\"][0:10]\n",
        "input_sequences = [i['en'] for i in batch]\n",
        "batch_output = [i['fr'] for i in batch]\n",
        "\n",
        "task_prefix = \"translate English to French: \"\n",
        "encoding = tok(\n",
        "    [task_prefix + sequence for sequence in input_sequences],\n",
        "    padding=\"longest\",\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = encoding.input_ids\n",
        "model_outputs = model.generate(input_ids)\n",
        "\n",
        "## converting model output into text (this is called tokenizer \"decoding\") (not to confuse with decoding algorithms like sampling and beam search these have the function model.generate)\n",
        "model_output_decoded = tok.batch_decode(model_outputs, skip_special_tokens=True)\n",
        "for s,t,g in zip(input_sequences, batch_output, model_output_decoded):\n",
        "  print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated\\t{}\\n------\\n\".format(s[0:1000],t,g))"
      ],
      "metadata": {
        "trusted": true,
        "id": "htEe7JdicVMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task1.1:Summarization**"
      ],
      "metadata": {
        "id": "F0JaXHwBcVMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = load_dataset(\"ccdv/cnn_dailymail\", '3.0.0',split=\"train\")\n",
        "val_data = load_dataset(\"ccdv/cnn_dailymail\", '3.0.0', split=\"validation[:10%]\")\n",
        "test_data =load_dataset(\"ccdv/cnn_dailymail\", '3.0.0', split=\"test\")\n",
        "sum_cnn_eval = test_data[0:1000]"
      ],
      "metadata": {
        "trusted": true,
        "id": "G3IoHeX2cVMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "#model = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-cnn_dailymail\").to(\"cuda\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"sshleifer/distilbart-xsum-6-6\")\n",
        "\n",
        "# Tokenizer\n",
        "tok = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "batch_input = test_data[\"article\"][0:10]\n",
        "batch_output = test_data[\"highlights\"][0:10]\n",
        "\n",
        "# using our loaded tokenizer: we will encode the input documents into \n",
        "# max_length is 1024 as the bart model allows to accept 1024 tokens max as an input \n",
        "input_encodings = tok.batch_encode_plus(batch_input, padding=True, max_length=1024, truncation=True, return_tensors=\"pt\")\n",
        "target_encodings = tok.batch_encode_plus(batch_output, padding=True, max_length=1024, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Given the batch Decode the answer from your model note that model.generate function takes many params, it will operate as greedy decoding if none is given. \n",
        "model_output = model.generate(input_encodings[\"input_ids\"])\n",
        "\n",
        "## converting model output into text (this is called tokenizer \"decoding\") (not to confuse with decoding algorithms like sampling and beam search these have the function model.generate)\n",
        "model_output_decoded = tok.batch_decode(model_output, skip_special_tokens=True)\n",
        "for s,t,g in zip(batch_input, batch_output, model_output_decoded):\n",
        "  print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated\\t{}\\n------\\n\".format(s[0:1000],t,g))"
      ],
      "metadata": {
        "trusted": true,
        "id": "EZcXV0BvcVMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task1.1:Question Answering**"
      ],
      "metadata": {
        "id": "CyOZZc3kcVME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading dataset\n",
        "valid_dataset = load_dataset(\"boolq\", split = \"train\")\n",
        "qa_eval = valid_dataset[0:1000]"
      ],
      "metadata": {
        "trusted": true,
        "id": "L23HKHZZcVME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boolq_dataset = load_dataset('boolq')\n",
        "\n",
        "batch = boolq_dataset['validation'][0:10]\n",
        "passages = batch[\"passage\"]\n",
        "questions = batch[\"question\"]\n",
        "answers = batch[\"answer\"]\n",
        "\n",
        "# Define a function to combine passages and questions\n",
        "def process_text(passage, question):\n",
        "    text = \"passage: <s> %s </s> question: %s\" % (passage, question)\n",
        "    return text \n",
        "\n",
        "context = [process_text(passage,question) for passage,question in zip(passages, questions)] \n",
        "\n",
        "tok.pad_token = tok.eos_token\n",
        "\n",
        "task_prefix = \"truefalse: \"\n",
        "encoding = tok(\n",
        "                   [task_prefix + sequence for sequence in context],\n",
        "                    max_length=512,\n",
        "                    padding=\"max_length\",\n",
        "                    truncation=\"only_second\",\n",
        "                    add_special_tokens=True,\n",
        "                    return_tensors=\"pt\")\n",
        "\n",
        "# encoding = tokenizer(passages, questions,return_tensors = \"pt\")\n",
        "input_ids = encoding[\"input_ids\"]\n",
        "attention_masks = encoding[\"attention_mask\"]\n",
        "\n",
        "model_output = model.generate(input_ids = input_ids,\n",
        "                                attention_mask = attention_masks,)\n",
        "\n",
        "## converting model output into text (this is called tokenizer \"decoding\") (not to confuse with decoding algorithms like sampling and beam search these have the function model.generate)\n",
        "model_output_decoded = tok.batch_decode(model_output, \n",
        "                                              skip_special_tokens=True)\n",
        "                                              \n",
        "for s,t,g in zip(passages, questions,model_output_decoded):\n",
        "  print(\"source:\\t {}\\nquestions:\\t {}\\ngenerated\\t{}\\n------\\n\".format(s[0:1000],t,g))"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ui8w3kLWcVME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task1.2- Implement Extra evaluation Metrics**"
      ],
      "metadata": {
        "id": "UQ-A4YGVcVMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement of Bleu**"
      ],
      "metadata": {
        "id": "lYJ_LzMOcVMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n",
        "!pip install mosestokenizer"
      ],
      "metadata": {
        "trusted": true,
        "id": "RngsuNNucVMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re \n",
        "import numpy as np \n",
        "from collections import Counter, namedtuple\n",
        "from tqdm import tqdm \n",
        "from datasets import load_metric\n",
        "import pandas as pd\n",
        "from mosestokenizer import * \n",
        "\n",
        "class BLEU:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.max_order = 4\n",
        "        self.BleuScore = namedtuple('BleuScore', ['bleu','precisions','bp','translation_length','reference_length'])\n",
        "\n",
        "    def _get_ngrams(self, sentence):\n",
        "        ngram_counts = Counter()\n",
        "        for i in range(1, self.max_order + 1):\n",
        "            for j in range(0, len(sentence) - i+1):\n",
        "                ngram = tuple(sentence[j:j+i])\n",
        "                ngram_counts[ngram] += 1\n",
        "        return ngram_counts\n",
        "\n",
        "    def bleu_on_corpus(self,ref,tran,smooth=bool):\n",
        "        ordered_matches, possible_matches = [0] * self.max_order, [0] * self.max_order\n",
        "        len_reference, len_translation = 0, 0\n",
        "\n",
        "        for (reference,translation) in zip(ref,tran):\n",
        "            len_reference += len(reference)\n",
        "            len_translation += len(translation)\n",
        "\n",
        "            merged_ngrams_ref = Counter()\n",
        "            for r in reference:\n",
        "                merged_ngrams_ref |= self._get_ngrams(reference)\n",
        "            merged_ngrams_trans = self._get_ngrams(translation)\n",
        "\n",
        "            #Get the overlap now \n",
        "            intersect = merged_ngrams_ref & merged_ngrams_trans\n",
        "            for grams in intersect:\n",
        "                ordered_matches[len(grams)-1] += intersect[grams]\n",
        "\n",
        "            #Obtain the dividend of the precision scores \n",
        "            for i in range(1, self.max_order + 1):\n",
        "                matches = len(translation) - i + 1\n",
        "                if matches > 0:\n",
        "                    possible_matches[i-1] += matches\n",
        "\n",
        "        #Obtain the modified precision \n",
        "        precision = [0] * self.max_order\n",
        "        for i in range(0, self.max_order):\n",
        "            if smooth:\n",
        "                precision[i] = ((ordered_matches[i] + 1.0) / \n",
        "                               (possible_matches[i] + 1.0))\n",
        "            else:\n",
        "                if possible_matches[i] > 0:\n",
        "                    precision[i] = (float(ordered_matches[i])/possible_matches[i])\n",
        "                else:\n",
        "                    precision[i] = 0.0\n",
        "\n",
        "        if min(precision) > 0:\n",
        "            geometric_mean = np.round(np.exp(sum((1.0/self.max_order) * np.log(p) for p in precision)),3)\n",
        "        else:\n",
        "            geometric_mean = 0 \n",
        "\n",
        "        #Brevity penalty \n",
        "        ratio = np.round(float(len_translation)/len_reference,2)\n",
        "        \n",
        "        if ratio > 1.0:\n",
        "            bp = 1.0\n",
        "        else:\n",
        "            bp = np.exp(1 - 1.0/ratio)\n",
        "\n",
        "        bleu = np.round(geometric_mean * bp, 4)\n",
        "\n",
        "        return self.BleuScore(bleu=bleu,precisions=precision,bp=bp,translation_length=len_translation,\n",
        "                             reference_length=len_reference)\n",
        "                                    \n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "vdTcwwmscVMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu(dataset):\n",
        "\n",
        "    input = dataset['translation'][0:1000]\n",
        "    batch_input = [input['en'] for input in input] \n",
        "    batch_output = [input['fr'] for input in input]\n",
        "    \n",
        "    task_prefix = \"translate English to French: \"\n",
        "    inputs = tok([task_prefix + sentence for sentence in batch_input],\n",
        "                       return_tensors=\"pt\", \n",
        "                       padding=True)\n",
        "    \n",
        "    model_output = model.generate(input_ids=inputs[\"input_ids\"],\n",
        "                                    attention_mask=inputs[\"attention_mask\"],\n",
        "                                    do_sample=False)\n",
        "    \n",
        "    output = tok.batch_decode(model_output, skip_special_tokens=True)\n",
        "\n",
        "    fr_tokenizer = MosesTokenizer('fr')\n",
        "    reference = []\n",
        "    prediction = []\n",
        "    for i in batch_output:\n",
        "        reference.append(fr_tokenizer(i))\n",
        "        \n",
        "    for i in output:\n",
        "        prediction.append(fr_tokenizer(i))\n",
        "    \n",
        "    # My bleu\n",
        "    bleu = BLEU()\n",
        "    my_bleu = bleu.bleu_on_corpus(reference,prediction,smooth=False)[0]\n",
        "\n",
        "    # Bleu inside huggingface\n",
        "    reference_snts = [[r] for r in reference]\n",
        "    bleu_metric = load_metric(\"bleu\")\n",
        "    true_bleu = np.round(bleu_metric.compute(predictions=prediction,references=reference_snts)['bleu'],4)\n",
        "\n",
        "    return my_bleu, true_bleu"
      ],
      "metadata": {
        "trusted": true,
        "id": "fDsuq2l8cVMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bible_dataset = load_dataset(\"bible_para\",lang1 = 'en', lang2 = 'fr', split = \"train[:20%]\")\n",
        "eu_dataset = load_dataset(\"ted_iwlst2013\", \"en-fr\", split = \"train[:1%]\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "RyiD9CxXcVMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_bleu_bible, true_bleu_bible = compute_bleu(bible_dataset)\n",
        "my_bleu_eu, true_bleu_eu = compute_bleu(eu_dataset)\n",
        "\n",
        "df_bleu = pd.DataFrame({\"HuggingFace\": [true_bleu_bible, true_bleu_eu],\n",
        "                         \"My result\": [my_bleu_bible, my_bleu_eu]}, index=['bible Dataset','europa_ecdc_tm Dataset'])\n",
        "\n",
        "df_bleu"
      ],
      "metadata": {
        "trusted": true,
        "id": "gnI8kM1PcVMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:30496f17-3b02-42d9-a89f-b2b30f22b91c.png)"
      ],
      "metadata": {
        "id": "57-U-1UScVMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation of rouge**"
      ],
      "metadata": {
        "id": "mYGlFhDIcVMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#N-gram function\n",
        "def generate_ngrams(s, n):\n",
        "    # Convert to lowercases\n",
        "    s = s.lower()\n",
        "    s = s.replace(\"'\",\"\")\n",
        "    \n",
        "    # Replace all none alphanumeric characters with spaces\n",
        "    s = re.sub(r'[^\\w\\s]', ' ', s)\n",
        "    s = re.sub(r'[^a-zA-Z0-9\\s]', '', s)\n",
        "    \n",
        "    # Break sentence in the token, remove empty tokens\n",
        "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
        "    \n",
        "    # Use the zip function to help us generate n-grams\n",
        "    # Concatentate the tokens into ngrams and return\n",
        "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "    return [\" \".join(ngram) for ngram in ngrams]\n",
        "\n",
        "#ROUGE metric \n",
        "def ROUGE(candidate=str, reference=str, n=int):\n",
        "\n",
        "    #Generate n-grams of the reference and candidate\n",
        "    reference_grams = Counter(generate_ngrams(reference,n))\n",
        "    candidate_grams = Counter(generate_ngrams(candidate,n))\n",
        "\n",
        "    intersect = reference_grams & candidate_grams\n",
        "    intersect = sum(intersect.values())\n",
        "\n",
        "    # Compute the precision,recall and F1 score \n",
        "    precision = intersect / (sum(candidate_grams.values())+1)\n",
        "    recall = intersect / sum(reference_grams.values())\n",
        "    f1 = 2*precision*recall/(precision+recall+1e-8)\n",
        "\n",
        "    return precision, recall, f1\n",
        "       "
      ],
      "metadata": {
        "trusted": true,
        "id": "X2wHwKz9cVMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare ROUGE score for the summarization \n",
        "def compute_rouge(dataset):\n",
        "\n",
        "    precision, recall, f1_ = [],[],[]\n",
        "    precision_rouge, recall_rouge, f1_rouge = [],[],[]\n",
        "\n",
        "    #Batch train in chunks of 10 to avoid memory overload\n",
        "    for i in tqdm(range(0,1000,10)):\n",
        "        test_input = dataset['test'][i:i+10]\n",
        "\n",
        "        batch_input= [input.strip(\"(CNN)\") for input in test_input['article']] \n",
        "        batch_output = [input.replace(\"\\n\", \"\") for input in test_input['highlights']]\n",
        "        \n",
        "        task_prefix = \"summarize: \"\n",
        "        inputs = tok([task_prefix + sentence for sentence in batch_input],\n",
        "                     max_length=512,\n",
        "                     padding=True,\n",
        "                     truncation=True,\n",
        "                      return_tensors=\"pt\")\n",
        "\n",
        "        model_output = model.generate(input_ids=inputs[\"input_ids\"])\n",
        "        output = tok.batch_decode(model_output, skip_special_tokens=True)\n",
        "\n",
        "        #HuggingFace implementation of ROUGE \n",
        "        rouge = load_metric(\"rouge\")\n",
        "        rouge = rouge.compute(predictions=output,references=batch_output)\n",
        "        precision_rouge.append(rouge[\"rouge2\"].mid.precision)\n",
        "        recall_rouge.append(rouge[\"rouge2\"].mid.recall)\n",
        "        f1_rouge.append(rouge[\"rouge2\"].mid.fmeasure)\n",
        "\n",
        "        #My ROUGE \n",
        "        for j in range(10):\n",
        "            pre,rec,f1 = ROUGE(output[j], batch_output[j], n=2)\n",
        "            precision.append(pre)\n",
        "            recall.append(rec)\n",
        "            f1_.append(f1)\n",
        "\n",
        "    return np.mean(precision) , np.mean(recall), np.mean(f1), np.mean(precision_rouge), np.mean(recall_rouge), np.mean(f1_rouge)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "WPiopiFAcVMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision,recall,f1,precision_rouge,recall_rouge,f1_rouge = compute_rouge(cnn_dataset)\n",
        "\n",
        "df_rouge_cnn = pd.DataFrame({\"HuggingFace (n=2)\":[precision_rouge,recall_rouge,f1_rouge],\n",
        "                         \"Hard Coded (n=2)\":[precision, recall,f1]},index=['Precision','Recall','F1'])\n",
        "\n",
        "df_rouge_cnn"
      ],
      "metadata": {
        "id": "tUmvTAv8cVMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:40873463-450b-4732-89bd-4e095fedb1e1.png)"
      ],
      "metadata": {
        "id": "h_nI-WekcVMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.3 - Implement Decoding metods on your own**"
      ],
      "metadata": {
        "id": "6lgwe447cVMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Table 2 calculation and implementation**"
      ],
      "metadata": {
        "id": "s4dJyJGzcVMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bible-para translation:Beam Search**"
      ],
      "metadata": {
        "id": "HySgns20cVMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model for bible_para dataset\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Tokenizer\n",
        "tok = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "num_return_sequences = 1\n",
        "idx = 3 \n",
        "\n",
        "valid_dataset = load_dataset(\"bible_para\",lang1 = 'en', lang2 = 'fr', split = \"train[:20%]\")\n",
        "trans_bible_eval = valid_dataset[0:1000]\n",
        "\n",
        "batch = valid_dataset['translation'][0:10]\n",
        "input_sequences = [i['en'] for i in batch]\n",
        "batch_output = [i['fr'] for i in batch]\n",
        "\n",
        "task_prefix = \"translate English to French: \"\n",
        "encoding = tok(\n",
        "    [task_prefix + sequence for sequence in input_sequences],\n",
        "    padding=\"longest\",\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = encoding.input_ids\n",
        "\n",
        "#Result given by Beam Search\n",
        "beam_outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=60, \n",
        "    num_beams= 10, \n",
        "    no_repeat_ngram_size=2, \n",
        "    num_return_sequences=num_return_sequences, \n",
        "    early_stopping=True,\n",
        "    output_scores=True \n",
        ")\n",
        "\n",
        "#Result given by T5 \n",
        "model_outputs = model.generate(input_ids, output_scores=True)\n",
        "\n",
        "model_output_decoded = tok.batch_decode(model_outputs, skip_special_tokens=True)\n",
        "\n",
        "beam_search_output_decoded = tok.batch_decode(beam_outputs, skip_special_tokens=True)\n",
        "\n",
        "for s,t,g_beam, g in zip(input_sequences, batch_output, beam_search_output_decoded, model_output_decoded):\n",
        "  print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated_beam\\t{}\\ngenerated\\t{}\\n------\\n\".format(s[0:1000],t,g_beam, g))"
      ],
      "metadata": {
        "trusted": true,
        "id": "aFUDYq9hcVMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking results for different beam lengths"
      ],
      "metadata": {
        "id": "Ct-QhfohcVMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bert_score\n",
        "import sacrebleu\n",
        "import pandas as pd "
      ],
      "metadata": {
        "trusted": true,
        "id": "JyCYcS4CcVMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nums_beans = [5,10,15,20,25,30,35,40,45,50]\n",
        "score_precision = []\n",
        "score_recall = []\n",
        "score_f1 = []\n",
        "for num_beans in nums_beans:\n",
        "  beam_outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=60, \n",
        "    num_beams= num_beans, \n",
        "    num_return_sequences=num_return_sequences, \n",
        "    early_stopping=True,\n",
        "    output_scores=True \n",
        "  )\n",
        "\n",
        "  beam_search_output_decoded = tok.batch_decode(beam_outputs, skip_special_tokens=True)\n",
        "\n",
        "  precision_beam,recall_beam,fscore_beam = bert_score.score(cands=beam_search_output_decoded, refs=batch_output, lang=\"en\")\n",
        "  score_precision.append(float(precision_beam.mean()))\n",
        "  score_recall.append(float(recall_beam.mean()))\n",
        "  score_f1.append(float(fscore_beam.mean()))\n",
        "\n",
        "df_beam = pd.DataFrame(list(zip(nums_beans, score_precision, score_recall, score_f1)), columns = [\"num_beams\", \"precision\", \"recall\", \"f1_score\"])\n",
        "\n",
        "df_beam"
      ],
      "metadata": {
        "trusted": true,
        "id": "vXcZQX2zcVML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarization-cnn dailymail dataset :Beam Search**"
      ],
      "metadata": {
        "id": "GCIq2Mw2cVML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = load_dataset(\"ccdv/cnn_dailymail\", '3.0.0',split=\"train\")\n",
        "val_data = load_dataset(\"ccdv/cnn_dailymail\", '3.0.0', split=\"validation[:10%]\")\n",
        "test_data =load_dataset(\"ccdv/cnn_dailymail\", '3.0.0', split=\"test\")\n",
        "sum_cnn_eval = test_data[0:100]\n",
        "\n",
        "num_return_sequences = 1\n",
        "idx = 3 \n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "tok = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "batch_input = test_data[\"article\"][0:10]\n",
        "batch_output = test_data[\"highlights\"][0:10]\n",
        "\n",
        "input_encodings = tok.batch_encode_plus(batch_input, padding=True, max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "target_encodings = tok.batch_encode_plus(batch_output, padding=True, max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "model_output = model.generate(input_encodings[\"input_ids\"], max_length = 60)\n",
        "\n",
        "beam_outputs = model.generate(\n",
        "    input_encodings[\"input_ids\"], \n",
        "    max_length=60, \n",
        "    num_beams= 10, \n",
        "    no_repeat_ngram_size=1, \n",
        "    num_return_sequences=num_return_sequences, \n",
        "    early_stopping=True \n",
        ")\n",
        "\n",
        "## converting model output into text (this is called tokenizer \"decoding\") (not to confuse with decoding algorithms like sampling and beam search these have the function model.generate)\n",
        "model_output_decoded = tok.batch_decode(model_output, skip_special_tokens=True)\n",
        "beam_search_output_decoded = tok.batch_decode(beam_outputs, skip_special_tokens=True)\n",
        "\n",
        "for s,t,g_beam, g in zip(batch_input, batch_output, beam_search_output_decoded, model_output_decoded):\n",
        "  print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated_beam\\t{}\\ngenerated\\t{}\\n------\\n\".format(s[0:1000],t,g_beam, g))"
      ],
      "metadata": {
        "trusted": true,
        "id": "MuM1_cUXcVML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking results for different beam lengths"
      ],
      "metadata": {
        "id": "5ZGJjf8ccVMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums_beans = [5,10,15,20]\n",
        "score_precision = []\n",
        "score_recall = []\n",
        "score_f1 = []\n",
        "for num_beans in nums_beans:\n",
        "  beam_outputs = model.generate(\n",
        "    input_encodings[\"input_ids\"], \n",
        "    max_length=60, \n",
        "    num_beams= num_beans, \n",
        "    no_repeat_ngram_size=1, \n",
        "    num_return_sequences=num_return_sequences, \n",
        "    early_stopping=True \n",
        "  ) \n",
        "\n",
        "  beam_search_output_decoded = tok.batch_decode(beam_outputs, skip_special_tokens=True)\n",
        "\n",
        "  precision_beam,recall_beam,fscore_beam = bert_score.score(cands=beam_search_output_decoded, refs=batch_output, lang=\"en\")\n",
        "  score_precision.append(float(precision_beam.mean()))\n",
        "  score_recall.append(float(recall_beam.mean()))\n",
        "  score_f1.append(float(fscore_beam.mean()))\n",
        "\n",
        "df_beam = pd.DataFrame(list(zip(nums_beans, score_precision, score_recall, score_f1)), columns = [\"num_beams\", \"precision\", \"recall\", \"f1_score\"])\n",
        "\n",
        "df_beam"
      ],
      "metadata": {
        "trusted": true,
        "id": "PlVkT3BmcVMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question Answering- BoolQ dataset:Beam Search**"
      ],
      "metadata": {
        "id": "uBg9_IYlcVMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boolq_dataset = load_dataset('boolq')\n",
        "\n",
        "batch = boolq_dataset['validation'][0:10]\n",
        "passages = batch[\"passage\"]\n",
        "questions = batch[\"question\"]\n",
        "answers = batch[\"answer\"]\n",
        "\n",
        "# Define a function to combine passages and questions\n",
        "def process_text(passage, question):\n",
        "    text = \"passage: <s> %s </s> question: %s\" % (passage, question)\n",
        "    return text \n",
        "\n",
        "context = [process_text(passage,question) for passage,question in zip(passages, questions)] \n",
        "\n",
        "tok.pad_token = tok.eos_token\n",
        "\n",
        "task_prefix = \"truefalse: \"\n",
        "encoding = tok(\n",
        "                   [task_prefix + sequence for sequence in context],\n",
        "                    max_length=512,\n",
        "                    padding=\"max_length\",\n",
        "                    truncation=\"only_second\",\n",
        "                    add_special_tokens=True,\n",
        "                    return_tensors=\"pt\")\n",
        "\n",
        "# encoding = tokenizer(passages, questions,return_tensors = \"pt\")\n",
        "input_ids = encoding[\"input_ids\"]\n",
        "attention_masks = encoding[\"attention_mask\"]\n",
        "\n",
        "model_output = model.generate(\n",
        "                                input_ids = input_ids,\n",
        "                                attention_mask = attention_masks, max_length = 60)\n",
        "beam_outputs = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask = attention_masks,\n",
        "    max_length=60, \n",
        "    num_beams= 10, \n",
        "    no_repeat_ngram_size=2, \n",
        "    num_return_sequences=num_return_sequences, \n",
        "    early_stopping=True \n",
        ")\n",
        "model_output_decoded = tok.batch_decode(model_output, skip_special_tokens=True)\n",
        "beam_search_output_decoded = tok.batch_decode(beam_outputs, skip_special_tokens=True)\n",
        "\n",
        "for s,t,g_beam, g in zip(questions, answers, beam_search_output_decoded, model_output_decoded):\n",
        "    print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated_beam\\t{}\\ngenerated\\t{}\\n------\\n\".format(s[0:1000],t,g_beam, g))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "vrzRC9BOcVMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking results for different beam lengths"
      ],
      "metadata": {
        "id": "8oWpqia5cVMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums_beans = [5,10,15,20]\n",
        "score_precision = []\n",
        "score_recall = []\n",
        "score_f1 = []\n",
        "for num_beans in nums_beans:\n",
        "  beam_outputs = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask = attention_masks,\n",
        "    max_length=60, \n",
        "    num_beams= num_beans, \n",
        "    no_repeat_ngram_size=1, \n",
        "    num_return_sequences=num_return_sequences, \n",
        "    early_stopping=True \n",
        "  ) \n",
        "\n",
        "  beam_search_output_decoded = tok.batch_decode(beam_outputs, skip_special_tokens=True)\n",
        "\n",
        "  precision_beam,recall_beam,fscore_beam = bert_score.score(cands=beam_search_output_decoded, refs=batch_output, lang=\"en\")\n",
        "  score_precision.append(float(precision_beam.mean()))\n",
        "  score_recall.append(float(recall_beam.mean()))\n",
        "  score_f1.append(float(fscore_beam.mean()))\n",
        "\n",
        "df_beam = pd.DataFrame(list(zip(nums_beans, score_precision, score_recall, score_f1)), columns = [\"num_beams\", \"precision\", \"recall\", \"f1_score\"])\n",
        "\n",
        "df_beam"
      ],
      "metadata": {
        "trusted": true,
        "id": "0xZbvaCYcVMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nucleus Sampling:Bible para translation**"
      ],
      "metadata": {
        "id": "k-Ff6D-JcVMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model for bible_para dataset\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Tokenizer\n",
        "tok = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "num_return_sequences = 1\n",
        "idx = 3 \n",
        "\n",
        "valid_dataset = load_dataset(\"bible_para\",lang1 = 'en', lang2 = 'fr', split = \"train[:20%]\")\n",
        "trans_bible_eval = valid_dataset[0:1000]\n",
        "\n",
        "batch = valid_dataset['translation'][0:10]\n",
        "input_sequences = [i['en'] for i in batch]\n",
        "batch_output = [i['fr'] for i in batch]\n",
        "\n",
        "task_prefix = \"translate English to French: \"\n",
        "encoding = tok(\n",
        "    [task_prefix + sequence for sequence in input_sequences],\n",
        "    padding=\"longest\",\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = encoding.input_ids\n",
        "\n",
        "#Result given by Beam Search\n",
        "nucleus_outputs = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_p=0.92, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "#Result given by T5 \n",
        "model_outputs = model.generate(input_ids, output_scores=True)\n",
        "\n",
        "model_output_decoded = tok.batch_decode(model_outputs, skip_special_tokens=True)\n",
        "\n",
        "nucleus_output_decoded = tokenizer.batch_decode(nucleus_outputs, skip_special_tokens=True)\n",
        "\n",
        "for s,t,g_nucleus, g in zip(input_sequences, batch_output, nucleus_output_decoded, model_output_decoded):\n",
        "  print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated_beam\\t{}\\ngenerated\\t{}\\n------\\n\".format(s[0:1000],t,g_nucleus, g))"
      ],
      "metadata": {
        "trusted": true,
        "id": "4S9sxIWscVMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking results for different top-p values"
      ],
      "metadata": {
        "id": "V4pqgyoccVMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_p = [0.70,0.75,0.80,0.85,0.90,0.95,1.0]\n",
        "score_precision = []\n",
        "score_recall = []\n",
        "score_f1 = []\n",
        "for p in top_p:\n",
        "    nucleus_outputs = model.generate(\n",
        "        input_ids, \n",
        "        do_sample=True, \n",
        "        max_length=50, \n",
        "        top_p=p, \n",
        "        top_k=0)\n",
        "    \n",
        "    nucleus_output_decoded = tokenizer.batch_decode(nucleus_outputs, skip_special_tokens=True)\n",
        "\n",
        "    precision_nucleus,recall_nucleus,fscore_nucleus = bert_score.score(cands=nucleus_output_decoded, refs=batch_output, lang=\"en\")\n",
        "    score_precision.append(float(precision_nucleus.mean()))\n",
        "    score_recall.append(float(recall_nucleus.mean()))\n",
        "    score_f1.append(float(fscore_nucleus.mean()))\n",
        "\n",
        "df_nucleus = pd.DataFrame(list(zip(top_p, score_precision, score_recall, score_f1)), columns = [\"top_p\", \"precision\", \"recall\", \"f1_score\"])\n",
        "\n",
        "df_nucleus"
      ],
      "metadata": {
        "trusted": true,
        "id": "b_QokpFjcVMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nucleus Sampling:Summarization**"
      ],
      "metadata": {
        "id": "yzTQU5_YcVMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = load_dataset(\"ccdv/cnn_dailymail\", '3.0.0',split=\"train\")\n",
        "val_data = load_dataset(\"ccdv/cnn_dailymail\", '3.0.0', split=\"validation[:10%]\")\n",
        "test_data =load_dataset(\"ccdv/cnn_dailymail\", '3.0.0', split=\"test\")\n",
        "sum_cnn_eval = test_data[0:100]\n",
        "\n",
        "num_return_sequences = 1\n",
        "idx = 3 \n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "tok = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "batch_input = test_data[\"article\"][0:10]\n",
        "batch_output = test_data[\"highlights\"][0:10]\n",
        "\n",
        "input_encodings = tok.batch_encode_plus(batch_input, padding=True, max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "target_encodings = tok.batch_encode_plus(batch_output, padding=True, max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "model_output = model.generate(input_encodings[\"input_ids\"], max_length = 60)\n",
        "\n",
        "nucleus_outputs = model.generate(\n",
        "    input_encodings[\"input_ids\"], \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_p=0.92, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "#Result given by T5 \n",
        "model_outputs = model.generate(input_encodings[\"input_ids\"], output_scores=True)\n",
        "\n",
        "model_output_decoded = tok.batch_decode(model_outputs, skip_special_tokens=True)\n",
        "\n",
        "nucleus_output_decoded = tok.batch_decode(nucleus_outputs, skip_special_tokens=True)\n",
        "\n",
        "for s,t,g_nucleus, g in zip(batch_input, batch_output, nucleus_output_decoded, model_output_decoded):\n",
        "  print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated_nucleus\\t{}\\ngenerated\\t{}\\n------\\n\".format(s[0:1000],t,g_nucleus, g))"
      ],
      "metadata": {
        "trusted": true,
        "id": "4plPxGUWcVMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking results for different top-p values"
      ],
      "metadata": {
        "id": "PES51tc3cVMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_p = [0.80,0.85,0.90,0.95]\n",
        "score_precision = []\n",
        "score_recall = []\n",
        "score_f1 = []\n",
        "for p in top_p:\n",
        "    nucleus_outputs = model.generate(\n",
        "        input_encodings[\"input_ids\"],\n",
        "        do_sample=True, \n",
        "        max_length=50, \n",
        "        top_p=p, \n",
        "        top_k=0)\n",
        "    \n",
        "    nucleus_output_decoded = tok.batch_decode(nucleus_outputs, skip_special_tokens=True)\n",
        "\n",
        "    precision_nucleus,recall_nucleus,fscore_nucleus = bert_score.score(cands=nucleus_output_decoded, refs=batch_output, lang=\"en\")\n",
        "    score_precision.append(float(precision_nucleus.mean()))\n",
        "    score_recall.append(float(recall_nucleus.mean()))\n",
        "    score_f1.append(float(fscore_nucleus.mean()))\n",
        "\n",
        "df_nucleus = pd.DataFrame(list(zip(top_p, score_precision, score_recall, score_f1)), columns = [\"top_p\", \"precision\", \"recall\", \"f1_score\"])\n",
        "\n",
        "df_nucleus"
      ],
      "metadata": {
        "trusted": true,
        "id": "706zCqJwcVMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nucleus Sampling:Question Answering**"
      ],
      "metadata": {
        "id": "kH9Zj9m1cVMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boolq_dataset = load_dataset('boolq')\n",
        "\n",
        "batch = boolq_dataset['validation'][0:10]\n",
        "passages = batch[\"passage\"]\n",
        "questions = batch[\"question\"]\n",
        "answers = batch[\"answer\"]\n",
        "\n",
        "# Define a function to combine passages and questions\n",
        "def process_text(passage, question):\n",
        "    text = \"passage: <s> %s </s> question: %s\" % (passage, question)\n",
        "    return text \n",
        "\n",
        "context = [process_text(passage,question) for passage,question in zip(passages, questions)] \n",
        "\n",
        "tok.pad_token = tok.eos_token\n",
        "\n",
        "task_prefix = \"truefalse: \"\n",
        "encoding = tok(\n",
        "                   [task_prefix + sequence for sequence in context],\n",
        "                    max_length=512,\n",
        "                    padding=\"max_length\",\n",
        "                    truncation=\"only_second\",\n",
        "                    add_special_tokens=True,\n",
        "                    return_tensors=\"pt\")\n",
        "\n",
        "# encoding = tokenizer(passages, questions,return_tensors = \"pt\")\n",
        "input_ids = encoding[\"input_ids\"]\n",
        "attention_masks = encoding[\"attention_mask\"]\n",
        "\n",
        "model_output = model.generate(\n",
        "                                input_ids = input_ids,\n",
        "                                attention_mask = attention_masks, max_length = 60)\n",
        "nucleus_outputs = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask = attention_masks,\n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_p=0.92, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "\n",
        "model_output_decoded = tok.batch_decode(model_output, skip_special_tokens=True)\n",
        "\n",
        "nucleus_output_decoded = tok.batch_decode(nucleus_outputs, skip_special_tokens=True)\n",
        "\n",
        "for s,t,g_nucleus, g in zip(questions, answers,nucleus_output_decoded, model_output_decoded):\n",
        "  print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated_nucleus\\t{}\\ngenerated\\t{}\\n------\\n\".format(s[0:1000],t,g_nucleus, g))"
      ],
      "metadata": {
        "trusted": true,
        "id": "EMaQu9RFcVMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking results for different top-p values"
      ],
      "metadata": {
        "id": "kgbXYL7XcVMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_p = [0.80,0.85,0.90,0.95]\n",
        "score_precision = []\n",
        "score_recall = []\n",
        "score_f1 = []\n",
        "for p in top_p:\n",
        "    nucleus_outputs = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask = attention_masks,\n",
        "        do_sample=True, \n",
        "        max_length=50, \n",
        "        top_p=p, \n",
        "        top_k=0)\n",
        "    \n",
        "    nucleus_output_decoded = tok.batch_decode(nucleus_outputs, skip_special_tokens=True)\n",
        "\n",
        "    precision_nucleus,recall_nucleus,fscore_nucleus = bert_score.score(cands=nucleus_output_decoded, refs=batch_output, lang=\"en\")\n",
        "    score_precision.append(float(precision_nucleus.mean()))\n",
        "    score_recall.append(float(recall_nucleus.mean()))\n",
        "    score_f1.append(float(fscore_nucleus.mean()))\n",
        "\n",
        "df_nucleus = pd.DataFrame(list(zip(top_p, score_precision, score_recall, score_f1)), columns = [\"top_p\", \"precision\", \"recall\", \"f1_score\"])\n",
        "\n",
        "df_nucleus"
      ],
      "metadata": {
        "trusted": true,
        "id": "xhGLcpkicVMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Softmax with temperature:Bible para translation**"
      ],
      "metadata": {
        "id": "u0-ao7tqcVMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model for bible_para dataset\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Tokenizer\n",
        "tok = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "num_return_sequences = 1\n",
        "idx = 3 \n",
        "\n",
        "valid_dataset = load_dataset(\"bible_para\",lang1 = 'en', lang2 = 'fr', split = \"train[:20%]\")\n",
        "trans_bible_eval = valid_dataset[0:1000]\n",
        "\n",
        "batch = valid_dataset['translation'][0:10]\n",
        "input_sequences = [i['en'] for i in batch]\n",
        "batch_output = [i['fr'] for i in batch]\n",
        "\n",
        "task_prefix = \"translate English to French: \"\n",
        "encoding = tok(\n",
        "    [task_prefix + sequence for sequence in input_sequences],\n",
        "    padding=\"longest\",\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = encoding.input_ids\n",
        "\n",
        "#Result given by Beam Search\n",
        "temperature_outputs = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0, \n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "#Result given by T5 \n",
        "model_outputs = model.generate(input_ids, output_scores=True)\n",
        "\n",
        "model_output_decoded = tok.batch_decode(model_outputs, skip_special_tokens=True)\n",
        "\n",
        "temperature_output_decoded = tokenizer.batch_decode(temperature_outputs, skip_special_tokens=True)\n",
        "\n",
        "for s,t,g_temperature, g in zip(input_sequences, batch_output, temperature_output_decoded, model_output_decoded):\n",
        "  print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated_beam\\t{}\\ngenerated\\t{}\\n------\\n\".format(s[0:1000],t,g_temperature, g))"
      ],
      "metadata": {
        "id": "I2J-qY8VcVMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking results for different temperature values"
      ],
      "metadata": {
        "id": "1bRwnxL1cVMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = [0.3,0.5,0.6,0.7,0.8]\n",
        "score_precision = []\n",
        "score_recall = []\n",
        "score_f1 = []\n",
        "for i in temp:\n",
        "    temperature_outputs = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0, \n",
        "    temperature=i)\n",
        "    \n",
        "    temperature_output_decoded = tokenizer.batch_decode(temperature_outputs, skip_special_tokens=True)\n",
        "\n",
        "    precision_temperature,recall_temperature,fscore_temperature = bert_score.score(cands=temperature_output_decoded, refs=batch_output, lang=\"en\")\n",
        "    score_precision.append(float(precision_temperature.mean()))\n",
        "    score_recall.append(float(recall_temperature.mean()))\n",
        "    score_f1.append(float(fscore_temperature.mean()))\n",
        "\n",
        "df_nucleus = pd.DataFrame(list(zip(temp, score_precision, score_recall, score_f1)), columns = [\"temperature\", \"precision\", \"recall\", \"f1_score\"])\n",
        "\n",
        "df_nucleus"
      ],
      "metadata": {
        "trusted": true,
        "id": "X73dRglUcVMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Softmax with temperature:Summarization**"
      ],
      "metadata": {
        "id": "3Ex3XJwQcVMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = load_dataset(\"ccdv/cnn_dailymail\", '3.0.0',split=\"train\")\n",
        "val_data = load_dataset(\"ccdv/cnn_dailymail\", '3.0.0', split=\"validation[:10%]\")\n",
        "test_data =load_dataset(\"ccdv/cnn_dailymail\", '3.0.0', split=\"test\")\n",
        "sum_cnn_eval = test_data[0:100]\n",
        "\n",
        "num_return_sequences = 1\n",
        "idx = 3 \n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "tok = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "batch_input = test_data[\"article\"][0:10]\n",
        "batch_output = test_data[\"highlights\"][0:10]\n",
        "\n",
        "input_encodings = tok.batch_encode_plus(batch_input, padding=True, max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "target_encodings = tok.batch_encode_plus(batch_output, padding=True, max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "model_output = model.generate(input_encodings[\"input_ids\"], max_length = 60)\n",
        "\n",
        "temperature_outputs = model.generate(\n",
        "    input_encodings[\"input_ids\"],\n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0, \n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "model_outputs = model.generate(input_encodings[\"input_ids\"], output_scores=True)\n",
        "\n",
        "model_output_decoded = tok.batch_decode(model_outputs, skip_special_tokens=True)\n",
        "\n",
        "temperature_output_decoded = tok.batch_decode(temperature_outputs, skip_special_tokens=True)\n",
        "\n",
        "for s,t,g_temperature, g in zip(batch_input, batch_output, temperature_output_decoded, model_output_decoded):\n",
        "  print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated_temp\\t{}\\ngenerated\\t{}\\n------\\n\".format(s[0:1000],t,g_temperature, g))"
      ],
      "metadata": {
        "trusted": true,
        "id": "9WttRUbhcVMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking results for different temperature values"
      ],
      "metadata": {
        "id": "EC8xIgr8cVMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = [0.3,0.5,0.6,0.7,0.8]\n",
        "score_precision = []\n",
        "score_recall = []\n",
        "score_f1 = []\n",
        "for i in temp:\n",
        "    temperature_outputs = model.generate(\n",
        "    input_encodings[\"input_ids\"],\n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0, \n",
        "    temperature=i)\n",
        "    \n",
        "    temperature_output_decoded = tok.batch_decode(temperature_outputs, skip_special_tokens=True)\n",
        "\n",
        "    precision_temperature,recall_temperature,fscore_temperature = bert_score.score(cands=temperature_output_decoded, refs=batch_output, lang=\"en\")\n",
        "    score_precision.append(float(precision_temperature.mean()))\n",
        "    score_recall.append(float(recall_temperature.mean()))\n",
        "    score_f1.append(float(fscore_temperature.mean()))\n",
        "\n",
        "df_nucleus = pd.DataFrame(list(zip(temp, score_precision, score_recall, score_f1)), columns = [\"temperature\", \"precision\", \"recall\", \"f1_score\"])\n",
        "\n",
        "df_nucleus"
      ],
      "metadata": {
        "trusted": true,
        "id": "55y-uiElcVMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RErUW9pscVMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Softmax with temperature:Question Answering**"
      ],
      "metadata": {
        "id": "G3nV-1VRcVMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boolq_dataset = load_dataset('boolq')\n",
        "\n",
        "batch = boolq_dataset['validation'][0:10]\n",
        "passages = batch[\"passage\"]\n",
        "questions = batch[\"question\"]\n",
        "answers = batch[\"answer\"]\n",
        "\n",
        "# Define a function to combine passages and questions\n",
        "def process_text(passage, question):\n",
        "    text = \"passage: <s> %s </s> question: %s\" % (passage, question)\n",
        "    return text \n",
        "\n",
        "context = [process_text(passage,question) for passage,question in zip(passages, questions)] \n",
        "\n",
        "tok.pad_token = tok.eos_token\n",
        "\n",
        "task_prefix = \"truefalse: \"\n",
        "encoding = tok(\n",
        "                   [task_prefix + sequence for sequence in context],\n",
        "                    max_length=512,\n",
        "                    padding=\"max_length\",\n",
        "                    truncation=\"only_second\",\n",
        "                    add_special_tokens=True,\n",
        "                    return_tensors=\"pt\")\n",
        "\n",
        "# encoding = tokenizer(passages, questions,return_tensors = \"pt\")\n",
        "input_ids = encoding[\"input_ids\"]\n",
        "attention_masks = encoding[\"attention_mask\"]\n",
        "\n",
        "model_output = model.generate(\n",
        "                                input_ids = input_ids,\n",
        "                                attention_mask = attention_masks, max_length = 60)\n",
        "temperature_outputs = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask = attention_masks,\n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0, \n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "model_outputs = model.generate(input_encodings[\"input_ids\"], output_scores=True)\n",
        "\n",
        "model_output_decoded = tok.batch_decode(model_output, skip_special_tokens=True)\n",
        "\n",
        "temperature_output_decoded = tok.batch_decode(temperature_outputs, skip_special_tokens=True)\n",
        "\n",
        "for s,t,g_temperature, g in zip(batch_input, batch_output, temperature_output_decoded, model_output_decoded):\n",
        "  print(\"source:\\t {}\\ntarget:\\t{}\\ngenerated_temp\\t{}\\ngenerated\\t{}\\n------\\n\".format(s[0:1000],t,g_temperature, g))"
      ],
      "metadata": {
        "trusted": true,
        "id": "4SKGb5HtcVMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking results for different temperature values**"
      ],
      "metadata": {
        "id": "xAHyk1VkcVMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = [0.3,0.5,0.6,0.7,0.8]\n",
        "score_precision = []\n",
        "score_recall = []\n",
        "score_f1 = []\n",
        "for i in temp:\n",
        "    temperature_outputs = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask = attention_masks,\n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0, \n",
        "    temperature=i)\n",
        "    \n",
        "    temperature_output_decoded = tok.batch_decode(temperature_outputs, skip_special_tokens=True)\n",
        "\n",
        "    precision_temperature,recall_temperature,fscore_temperature = bert_score.score(cands=temperature_output_decoded, refs=batch_output, lang=\"en\")\n",
        "    score_precision.append(float(precision_temperature.mean()))\n",
        "    score_recall.append(float(recall_temperature.mean()))\n",
        "    score_f1.append(float(fscore_temperature.mean()))\n",
        "\n",
        "df_temp = pd.DataFrame(list(zip(temp, score_precision, score_recall, score_f1)), columns = [\"temperature\", \"precision\", \"recall\", \"f1_score\"])\n",
        "\n",
        "df_temp"
      ],
      "metadata": {
        "trusted": true,
        "id": "8G4EqCO8cVMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 2\n",
        "SUMMARY**"
      ],
      "metadata": {
        "id": "2PE6c3N7cVMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![image.png](attachment:790ed1a5-8538-46a5-9fbe-dbc88edd600f.png)!"
      ],
      "metadata": {
        "id": "QAiRpwzKcVMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Report**"
      ],
      "metadata": {
        "id": "NfylDKRGcVMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations(see plots below for support):- For the beam search, initially the precision decreases by greater margins as beam size is increased but stagnates when the number of beams increases beyond a certain point(like 40 for translation dataset).f1-score and precision vary in the same manner for all beam search cases.For nucleus sampling,the precision usually peaks around 0.85-0.90 p-value.It is observed that for nucleus sampling,the precision,recall and f1 have close values for very high or very low top-p.The reverse usually happens to f1-score.For softmax with temperature, the cooler (lower temperature),the more the f1-score.The optimum temperatue for best performance is around 0.5.The most preferable algorithm for me is nucleus sampling. The precision,recall and f1 can be controlled at high top-p to achieve desirable high performance.**"
      ],
      "metadata": {
        "id": "niu4ynofcVMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:08700db0-fb6a-4f5d-b5db-f85aca264db2.png)"
      ],
      "metadata": {
        "id": "30jDT6tRcVMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beam search:Bible para translation plot"
      ],
      "metadata": {
        "id": "jnsizrZccVMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:ee95f3a1-1dde-4b65-9b97-79dce114d13f.png)"
      ],
      "metadata": {
        "id": "5VJNP8FrcVMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nucleus Sampling:Bible para dataset"
      ],
      "metadata": {
        "id": "0fxhbNVvcVMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:189d7b50-fa83-4ad5-b43d-9da66a7ee862.png)"
      ],
      "metadata": {
        "id": "NaWvGB0zcVMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax with temperature:Bible para dataset plot"
      ],
      "metadata": {
        "id": "wX_P8RIjcVMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Table 1 calculation**"
      ],
      "metadata": {
        "id": "CUJ0bd24cVMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation of Beam Search**"
      ],
      "metadata": {
        "id": "KeRA5aSwcVMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "valid_dataset = load_dataset(\"bible_para\",lang1 = 'en', lang2 = 'fr', split = \"train[:20%]\")\n",
        "trans_bible_eval = valid_dataset[0:1000]\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Tokenizer\n",
        "tok = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "batch = valid_dataset['translation'][0:10]\n",
        "input_sequences = [i['en'] for i in batch]\n",
        "batch_output = [i['fr'] for i in batch]\n",
        "\n",
        "task_prefix = \"translate English to French: \"\n",
        "encoding = tok(\n",
        "    [task_prefix + sequence for sequence in input_sequences],\n",
        "    padding=\"longest\",\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = encoding.input_ids"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-15T22:42:22.218910Z",
          "iopub.execute_input": "2022-04-15T22:42:22.219782Z",
          "iopub.status.idle": "2022-04-15T22:42:24.740516Z",
          "shell.execute_reply.started": "2022-04-15T22:42:22.219733Z",
          "shell.execute_reply": "2022-04-15T22:42:24.739564Z"
        },
        "trusted": true,
        "id": "VFoQzLS9cVMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterations =10\n",
        "num_beams=5\n",
        "result = []\n",
        "\n",
        "def beam_search(model, inputs, iter=10, num_beams=5):\n",
        "  result=[]\n",
        "  for input_id in input_ids:\n",
        "    seq = [(1,[0])]\n",
        "    for i in range(iter):\n",
        "      \n",
        "      enc_input = input_id.repeat(len(seq), 1)\n",
        "      dec_input = torch.tensor([ids for _, ids in seq])\n",
        "      model\n",
        "\n",
        "      outputs = model(enc_input, decoder_input_ids = dec_input) \n",
        "      logits = outputs.logits[:,-1,:]\n",
        "      probs = torch.softmax(logits, 1) \n",
        "      top_beam = probs.topk(num_beams, 1) \n",
        "\n",
        "      temp = []\n",
        "      for i, (score, ids) in enumerate(seq):\n",
        "        for k in range(num_beams):\n",
        "          temp.append((score*top_beam.values[i,k].item(), ids + [top_beam.indices[i,k].item()]))\n",
        "          seq = sorted(temp,reverse = True)[:num_beams]  \n",
        "    result.append([tok.decode(torch.tensor(ids), skip_special_tokes=True) for _, ids in seq][0]) \n",
        "\n",
        "  return result\n",
        "\n",
        "model_output_decoded = beam_search(model,input_ids, iter=10, num_beams=5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ogDXPkydcVMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "EegwbWnmcVMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_moses = MosesTokenizer('en')\n",
        "rouge = load_metric('rouge')\n",
        "percision,recall,fscore = bert_score.score(cands=model_output_decoded, refs=batch_output ,lang=\"en\")\n",
        "bleu = sacrebleu.corpus_bleu(model_output_decoded, [batch_output]).score #machine translation\n",
        "\n",
        "\n",
        "df = pd.DataFrame({\"dataset\": 'bible',\n",
        "    \"BLEU\":bleu,\n",
        "    \"BERTSCORE-percision\": [float(percision.mean())],\n",
        "    \"BERTSCORE-recall\": [float(recall.mean())],\n",
        "    \"BERTSCORE-fscore\": [float(fscore.mean())]\n",
        "    })\n",
        "\n",
        "df"
      ],
      "metadata": {
        "trusted": true,
        "id": "PmbyWrk4cVMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation of Nucleus Sampling**"
      ],
      "metadata": {
        "id": "dJW6nFHbcVMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t=5\n",
        "iterations=10\n",
        "result = []\n",
        "\n",
        "for input_id in input_ids:\n",
        "  seq = [0]\n",
        "  enc_input = input_id.repeat(len(seq), 1)    \n",
        "  for i in range(iterations):\n",
        "    dec_input = torch.tensor([seq])\n",
        "    model\n",
        "\n",
        "    outputs = model(enc_input, decoder_input_ids = dec_input)\n",
        "    sorted, idx = torch.sort(outputs[0][0,-1,:], descending=True)\n",
        "    sorted = torch.softmax(sorted, dim=0)\n",
        "    cumsum_probs = torch.cumsum(sorted, 0)\n",
        "    probs_p = (cumsum_probs < t).nonzero()\n",
        "    temp = 1 if len(probs_p)==0 else probs_p[-1][0].item() + 2\n",
        "    temp = torch.multinomial(sorted[:temp], 1)\n",
        "    seq.append(idx[temp].item())\n",
        "  result.append(tok.decode(torch.tensor(seq), skip_special_tokes=True))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-15T21:47:13.698594Z",
          "iopub.execute_input": "2022-04-15T21:47:13.699479Z",
          "iopub.status.idle": "2022-04-15T21:47:20.365250Z",
          "shell.execute_reply.started": "2022-04-15T21:47:13.699422Z",
          "shell.execute_reply": "2022-04-15T21:47:20.364347Z"
        },
        "trusted": true,
        "id": "5516XHYQcVMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_t5_output_decoded = result\n",
        "tok_moses = MosesTokenizer('en')\n",
        "rouge = load_metric('rouge')\n",
        "percision,recall,fscore = bert_score.score(cands=model_t5_output_decoded, refs=batch_output, lang=\"en\")\n",
        "bleu = sacrebleu.corpus_bleu(model_output_decoded, [batch_output]).score \n",
        "\n",
        "df = pd.DataFrame({\"dataset\": 'bible',\n",
        "    \"BLEU\":bleu,\n",
        "    \"BERTSCORE-percision\": [float(percision.mean())],\n",
        "    \"BERTSCORE-recall\": [float(recall.mean())],\n",
        "    \"BERTSCORE-fscore\": [float(fscore.mean())]\n",
        "    })\n",
        "\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-15T21:50:15.452082Z",
          "iopub.execute_input": "2022-04-15T21:50:15.452449Z",
          "iopub.status.idle": "2022-04-15T21:50:26.328728Z",
          "shell.execute_reply.started": "2022-04-15T21:50:15.452411Z",
          "shell.execute_reply": "2022-04-15T21:50:26.327693Z"
        },
        "trusted": true,
        "id": "k5Z1-4iGcVMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation of Softmax with Temperature**"
      ],
      "metadata": {
        "id": "6mfCVJBXcVMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "T = 2\n",
        "iterations=10\n",
        "\n",
        "for input_id in input_ids: \n",
        "  seq = [0]\n",
        "  enc_input = input_id.repeat(len(seq), 1)\n",
        "  for i in range(iterations):\n",
        "    dec_input =torch.tensor([seq])\n",
        "    outputs = model(enc_input, decoder_input_ids=dec_input)\n",
        "    sorted, idx = torch.sort(outputs[0][0,-1,:], descending=True)\n",
        "    sorted = torch.softmax(sorted/T, 0)\n",
        "    temp = torch.multinomial(sorted, 1)\n",
        "    seq.append(idx[temp].item())\n",
        "  result.append(tok.decode(torch.tensor(seq), skip_special_tokes=True))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-15T21:53:00.786724Z",
          "iopub.execute_input": "2022-04-15T21:53:00.787808Z",
          "iopub.status.idle": "2022-04-15T21:53:09.223122Z",
          "shell.execute_reply.started": "2022-04-15T21:53:00.787731Z",
          "shell.execute_reply": "2022-04-15T21:53:09.222235Z"
        },
        "trusted": true,
        "id": "cQ54Pw6_cVMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_output_decoded = result\n",
        "tok_moses = MosesTokenizer('en')\n",
        "rouge = load_metric('rouge')\n",
        "\n",
        "bleu = sacrebleu.corpus_bleu(model_output_decoded, [batch_output]).score \n",
        "scores_rouge = rouge.compute(predictions = model_output_decoded, references= batch_output) \n",
        "\n",
        "\n",
        "df = pd.DataFrame({\"dataset\": 'bible',\n",
        "    \"BLEU\":bleu,\n",
        "    \"BERTSCORE-percision\": [float(percision.mean())],\n",
        "    \"BERTSCORE-recall\": [float(recall.mean())],\n",
        "    \"BERTSCORE-fscore\": [float(fscore.mean())]\n",
        "    })\n",
        "\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-15T21:54:46.583170Z",
          "iopub.execute_input": "2022-04-15T21:54:46.583517Z",
          "iopub.status.idle": "2022-04-15T21:54:47.317991Z",
          "shell.execute_reply.started": "2022-04-15T21:54:46.583473Z",
          "shell.execute_reply": "2022-04-15T21:54:47.316987Z"
        },
        "trusted": true,
        "id": "Apv4Uv-kcVMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 1 SUMMARY**"
      ],
      "metadata": {
        "id": "kclKRuvHcVMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:785fd27b-d507-4915-bb32-f436bad16d6c.png)"
      ],
      "metadata": {
        "id": "xtQiM7TjcVMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2.1**"
      ],
      "metadata": {
        "id": "YzEPme7XcVMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertviz\n",
        "!pip install datasets\n",
        "!pip install sacrerouge sacrebleu bert-score\n",
        "\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!pip install ./transformers/.\n",
        "!pip install SentencePiece==0.1.95"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-15T22:12:28.434695Z",
          "iopub.execute_input": "2022-04-15T22:12:28.435814Z",
          "iopub.status.idle": "2022-04-15T22:14:08.014474Z",
          "shell.execute_reply.started": "2022-04-15T22:12:28.435762Z",
          "shell.execute_reply": "2022-04-15T22:14:08.013172Z"
        },
        "trusted": true,
        "id": "7nHRgkT6cVMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, utils\n",
        "from bertviz import model_view, head_view\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5EncoderModel\n",
        "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
        "\n",
        "model_name = \"t5-small\" \n",
        "input_text = 'Fear is the path to the dark side. Fear leads to anger. Anger leads to hate. Hate leads to suffering.' \n",
        "label_text = 'La peur est le chemin vers le côté obscur : la peur mène à la colère, la colère mène à la haine, la haine mène à la souffrance.'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name, output_attentions=True)  # Configure model to return attention values\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "input_ids = tokenizer(input_text, return_tensors='pt').input_ids  # Tokenize input text\n",
        "labels = tokenizer(label_text, return_tensors='pt').input_ids\n",
        "outputs = model(input_ids = input_ids, labels = labels, output_attentions = True)  # Run model\n",
        "attention = outputs[-1]  # Retrieve attention from model outputs\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])  # Convert input ids to token strings\n",
        "model_view(attention, tokens)  # Display model view"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-15T22:14:08.016717Z",
          "iopub.execute_input": "2022-04-15T22:14:08.017077Z",
          "iopub.status.idle": "2022-04-15T22:14:10.136432Z",
          "shell.execute_reply.started": "2022-04-15T22:14:08.017035Z",
          "shell.execute_reply": "2022-04-15T22:14:10.130214Z"
        },
        "trusted": true,
        "id": "M21kBiTncVMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head_view(attention, tokens)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-15T22:14:38.885046Z",
          "iopub.execute_input": "2022-04-15T22:14:38.885345Z",
          "iopub.status.idle": "2022-04-15T22:14:38.963598Z",
          "shell.execute_reply.started": "2022-04-15T22:14:38.885315Z",
          "shell.execute_reply": "2022-04-15T22:14:38.962338Z"
        },
        "trusted": true,
        "id": "LhpB0c0XcVMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "for i in range(8):\n",
        "  fig, axs = plt.subplots(figsize=(8, 5))\n",
        "  sns.heatmap(attention[0][0][i].detach().numpy())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-15T22:14:43.063037Z",
          "iopub.execute_input": "2022-04-15T22:14:43.063689Z",
          "iopub.status.idle": "2022-04-15T22:14:46.965074Z",
          "shell.execute_reply.started": "2022-04-15T22:14:43.063631Z",
          "shell.execute_reply": "2022-04-15T22:14:46.964077Z"
        },
        "trusted": true,
        "id": "xV_EvtNqcVMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Report: The first thing we observe is that most of the words refer to the end of sentence symbol, whatever visualization we chose. Then, it appears (more clearly on the last representation) that each word refers mostly to the word right after it or right before it. Most of the layers show a diagonal pattern, which means that words are considered independent from one another. This can hint that some layers have more impact than others in the traduction process. Hence the proposition from 'Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?' article to 'prune' the attentions.**"
      ],
      "metadata": {
        "id": "NbuSBN2tcVMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2.2**"
      ],
      "metadata": {
        "id": "2xkhLQ9JcVMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confidence = np.delete(np.delete(attention[0][0][0].detach().numpy(), -1, 1), -1, 0)\n",
        "for layer in range(1, 8):\n",
        "  confidence += np.delete(np.delete(attention[0][0][layer].detach().numpy(), -1, 1), -1, 0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-15T22:14:59.345043Z",
          "iopub.execute_input": "2022-04-15T22:14:59.345382Z",
          "iopub.status.idle": "2022-04-15T22:14:59.353287Z",
          "shell.execute_reply.started": "2022-04-15T22:14:59.345349Z",
          "shell.execute_reply": "2022-04-15T22:14:59.352043Z"
        },
        "trusted": true,
        "id": "TFVUOb_5cVMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(confidence/8)"
      ],
      "metadata": {
        "trusted": true,
        "id": "h-Y-In3acVMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Report: Here we chose to represent the confidence aggregation method. The confidence here is the mean of attention over heads layer-wise. Thanks to confidence, we observe that the word 'anger' is considered rather independent while 'path' relates mostly to itself and to 'to'. This aggregation visualization is less precise than the previous observations but it helps understand the decision process better, which is the aim of attention. The results are even more flagrant and easier to interpret for a shorter sentence.**"
      ],
      "metadata": {
        "id": "am-CMeHhcVMd"
      }
    }
  ]
}